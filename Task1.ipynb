{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jora\n",
      "[nltk_data]    |     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jora\n",
      "[nltk_data]     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jora\n",
      "[nltk_data]     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jora\n",
      "[nltk_data]     Ismaili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('popular')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # This is the correct resource\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Sometimes the power of Christmas will make you...   \n",
      "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
      "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
      "3  When a rare shark was caught, scientists were ...   \n",
      "4  Donald Trump has the unnerving ability to abil...   \n",
      "\n",
      "                              manual_cleaned_content  \n",
      "0  sometimes the power of christmas will make you...  \n",
      "1  awakening of NUM strands of dna – “reconnectin...  \n",
      "2  never hike alone: a friday the NUM fan film us...  \n",
      "3  when a rare shark was caught, scientists were ...  \n",
      "4  donald trump has the unnerving ability to abil...  \n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"news_sample.csv\")  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()                                             # convert to lowercase\n",
    "    spaces = re.compile(r'\\s+')\n",
    "    text = spaces.sub(' ', text)                                    # substitute all white space characters (single or multiple occurences) with a single space\n",
    "\n",
    "    emails = re.compile(r'\\S+@\\S+\\.\\S+')\n",
    "    text = emails.sub('EMAIL', text)                              # substitute all found email addresses with EMAIL\n",
    "    urls = re.compile(r'http[s]?:\\/\\/\\S+|www\\.\\S+|\\S+\\.[a-z]+\\/\\S+|\\w+\\.(?:com|net|org)')\n",
    "    text = urls.sub('URL', text)                                  # substitute all found URLs with URL\n",
    "    dates = re.compile(r'''\n",
    "                       \\d{1,4}[-\\/]\\d{1,2}[-\\/]\\d{1,4}|\n",
    "                       \\d{1,2}\\ (?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[e]?|jul[y]?|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\\ \\d{,4}|\n",
    "                       (?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[e]?|jul[y]?|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)[,.]?\\ ?\\d{1,4}(?:th|st|nd|rd)?(?:,\\ \\d{4})?\n",
    "                       ''', re.VERBOSE)\n",
    "    text = dates.sub('DATE', text)                                # substitute all found dates with DATE\n",
    "    numbers = re.compile(r'\\d+(?:th|st|nd|rd)?')\n",
    "    text = numbers.sub('NUM', text)                               # substitute all remaining numbers with NUM\n",
    "    return text\n",
    "\n",
    "sample[\"manual_cleaned_content\"] = sample[\"content\"].apply(clean_text)\n",
    "print(sample[[\"content\", \"manual_cleaned_content\"]].head())         # printing a preview of the raw and processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              manual_cleaned_content  \\\n",
      "0  sometimes the power of christmas will make you...   \n",
      "1  awakening of NUM strands of dna – “reconnectin...   \n",
      "2  never hike alone: a friday the NUM fan film us...   \n",
      "3  when a rare shark was caught, scientists were ...   \n",
      "4  donald trump has the unnerving ability to abil...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [sometimes, the, power, of, christmas, will, m...  \n",
      "1  [awakening, of, NUM, strands, of, dna, reconne...  \n",
      "2  [never, hike, alone, a, friday, the, NUM, fan,...  \n",
      "3  [when, a, rare, shark, was, caught, scientists...  \n",
      "4  [donald, trump, has, the, unnerving, ability, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [word for word in tokens if word.isalnum() or word.startswith(\"_\")]  # Keep only words and numbers\n",
    "\n",
    "# Apply the tokenization function to your DataFrame (assuming `sample` is your DataFrame)\n",
    "sample[\"tokens\"] = sample[\"manual_cleaned_content\"].apply(tokenize)\n",
    "\n",
    "print(sample[[\"manual_cleaned_content\", \"tokens\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0  [sometimes, the, power, of, christmas, will, m...   \n",
      "1  [awakening, of, NUM, strands, of, dna, reconne...   \n",
      "2  [never, hike, alone, a, friday, the, NUM, fan,...   \n",
      "3  [when, a, rare, shark, was, caught, scientists...   \n",
      "4  [donald, trump, has, the, unnerving, ability, ...   \n",
      "\n",
      "                                 tokens_no_stopwords  \n",
      "0  [sometimes, power, christmas, make, wild, wond...  \n",
      "1  [awakening, NUM, strands, dna, reconnecting, m...  \n",
      "2  [never, hike, alone, friday, NUM, fan, film, u...  \n",
      "3  [rare, shark, caught, scientists, left, blunde...  \n",
      "4  [donald, trump, unnerving, ability, ability, c...  \n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))  # load English stopwords from NLTK\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "sample[\"tokens_no_stopwords\"] = sample[\"tokens\"].apply(remove_stopwords)\n",
    "print(sample[[\"tokens\", \"tokens_no_stopwords\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 tokens_no_stopwords  \\\n",
      "0  [sometimes, power, christmas, make, wild, wond...   \n",
      "1  [awakening, NUM, strands, dna, reconnecting, m...   \n",
      "2  [never, hike, alone, friday, NUM, fan, film, u...   \n",
      "3  [rare, shark, caught, scientists, left, blunde...   \n",
      "4  [donald, trump, unnerving, ability, ability, c...   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "0  [sometim, power, christma, make, wild, wonder,...  \n",
      "1  [awaken, num, strand, dna, reconnect, movi, re...  \n",
      "2  [never, hike, alon, friday, num, fan, film, us...  \n",
      "3  [rare, shark, caught, scientist, left, blunder...  \n",
      "4  [donald, trump, unnerv, abil, abil, creat, rea...  \n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "sample[\"stemmed_tokens\"] = sample[\"tokens_no_stopwords\"].apply(stem_words)\n",
    "print(sample[[\"tokens_no_stopwords\", \"stemmed_tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary before removing stopwords: 14993\n",
      "Vocabulary after removing stopwords: 14847\n",
      "Reduction rate after removing stopwords: 0.97%\n",
      "Vocabulary after stemming: 9704\n",
      "Reduction rate after stemming: 34.64%\n"
     ]
    }
   ],
   "source": [
    "size_tokenized = len(set(word for doc in sample[\"tokens\"] for word in doc))\n",
    "size_wo_stopwords = len(set(word for doc in sample[\"tokens_no_stopwords\"] for word in doc))\n",
    "size_stemmed = len(set(word for doc in sample[\"stemmed_tokens\"] for word in doc))\n",
    "\n",
    "stopword_reduction_rate = (size_tokenized - size_wo_stopwords) / size_tokenized * 100\n",
    "stemmed_reduction_rate = (size_wo_stopwords - size_stemmed) / size_wo_stopwords * 100\n",
    "\n",
    "print(f\"Vocabulary before removing stopwords: {size_tokenized}\")\n",
    "print(f\"Vocabulary after removing stopwords: {size_wo_stopwords}\")\n",
    "print(f\"Reduction rate after removing stopwords: {stopword_reduction_rate:.2f}%\")\n",
    "\n",
    "print(f\"Vocabulary after stemming: {size_stemmed}\")\n",
    "print(f\"Reduction rate after stemming: {stemmed_reduction_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (200,)\n",
      "X_val shape: (25,)\n",
      "X_test shape: (25,)\n",
      "\n",
      "y_train shape: (200,)\n",
      "y_val shape: (25,)\n",
      "y_test shape: (25,)\n",
      "\n",
      "Validation Set Sample:\n",
      " 135    [paean, wife, headlin, bitcoin, blockchain, se...\n",
      "205    [live, world, realiti, time, travel, appear, o...\n",
      "90     [cedar, bayou, fm, num, clear, creek, dayton, ...\n",
      "95     [democrat, hous, select, committe, benghazi, s...\n",
      "84     [russia, respond, us, provoc, open, sky, treat...\n",
      "Name: stemmed_tokens, dtype: object\n",
      "\n",
      "Test Set Sample:\n",
      " 174    [benzalkonium, chlorid, industri, num, global,...\n",
      "41     [excerpt, carl, sagan, cosmo, specif, episod, ...\n",
      "108    [give, soro, group, million, destabil, macedon...\n",
      "115    [donald, trump, part, new, world, order, quest...\n",
      "47     [drain, swamp, matter, long, groupthink, persi...\n",
      "Name: stemmed_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X (features) and y (labels)\n",
    "X = sample[\"stemmed_tokens\"]            # Feature data\n",
    "y = sample[\"type\"]                      # Target data\n",
    "\n",
    "# Step 1: Split into 80% Train and 20% Temp (Val + Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=104, shuffle=True)\n",
    "\n",
    "# Step 2: Split 20% Temp into 10% Val and 10% Test\n",
    "X_val, X_test_final, y_val, y_test_final = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print Shapes\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test_final.shape)\n",
    "\n",
    "print('')\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('y_test shape:', y_test_final.shape)\n",
    "\n",
    "# Print Samples\n",
    "print(\"\\nValidation Set Sample:\\n\", X_val.head())\n",
    "print(\"\\nTest Set Sample:\\n\", X_test_final.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
