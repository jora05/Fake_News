{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv')                                              # Reads train_data csv file created earlier to DataFrame\n",
    "validation_data = pd.read_csv('validation_data.csv')                                    # Reads validation_data csv file created earlier to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609283, 10000)\n",
      "(76160, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Extract content and labels\n",
    "X_train = train_data['stemmed_tokens']\n",
    "y_train = train_data['type']\n",
    "X_val = validation_data['stemmed_tokens']\n",
    "y_val = validation_data['type']\n",
    "\n",
    "# Convert text to a bag-of-words representation\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Use 10.000 most used words as features\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_val_bow = vectorizer.transform(X_val)\n",
    "\n",
    "# Check shape of resulting matrix (should be (sample_size, 10000))\n",
    "print(X_train_bow.shape)\n",
    "print(X_val_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1500 er nok til at den konvergerer, den bliver ikke bedre af 2000 vs 1500 :)) \\\n",
    "Jeg har ikke fjernet satire i Part1Task4 :)) Så stor forskel gør det alligevel ikke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8425\n",
      "F1 Score: 0.8369\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85     39503\n",
      "           1       0.83      0.84      0.84     36657\n",
      "\n",
      "    accuracy                           0.84     76160\n",
      "   macro avg       0.84      0.84      0.84     76160\n",
      "weighted avg       0.84      0.84      0.84     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1500) \n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 optimeret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8424\n",
      "F1 Score: 0.8393\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85     39503\n",
      "           1       0.82      0.86      0.84     36657\n",
      "\n",
      "    accuracy                           0.84     76160\n",
      "   macro avg       0.84      0.84      0.84     76160\n",
      "weighted avg       0.84      0.84      0.84     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "# Her med class-weight=balanced\n",
    "model = LogisticRegression(max_iter=1500, class_weight='balanced') \n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 mere optimeret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8423\n",
      "F1 Score: 0.8392\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85     39503\n",
      "           1       0.82      0.85      0.84     36657\n",
      "\n",
      "    accuracy                           0.84     76160\n",
      "   macro avg       0.84      0.84      0.84     76160\n",
      "weighted avg       0.84      0.84      0.84     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "# Her med class-weight=balanced og C=8.0\n",
    "model = LogisticRegression(max_iter=1500, class_weight='balanced', C=8.0) \n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 med domain som features (ud over main-text selvfølgelig) - er det snyd? Det er meget muligt. Men de skriver ikke, at det er ulovligt. Så vi kunne evt. skrive, at det er snyd at bruge domain som features, når det er det, der er brugt til at definere label'sne - og derfor modellen bliver så helt ekstremt god, fordi den basically får svarene givet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Matrix Shape (training): (609283, 10524)\n",
      "Final Feature Matrix Shape (validation): (76160, 10524)\n"
     ]
    }
   ],
   "source": [
    "# Encode 'domain' as a numerical feature\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # Handle unseen domains gracefully\n",
    "X_train_domain = encoder.fit_transform(train_data[['domain']])\n",
    "X_val_domain = encoder.transform(validation_data[['domain']])\n",
    "\n",
    "# Combine text features and domain features\n",
    "X_train_combined = hstack([X_train_bow, X_train_domain])                # Horizontally stack sparse matrices\n",
    "X_val_combined = hstack([X_val_bow, X_val_domain])                      # Horizontally stack sparse matrices\n",
    "\n",
    "# Check the resulting feature shape\n",
    "print(f\"Final Feature Matrix Shape (training): {X_train_combined.shape}\")  # (rows, num_features)\n",
    "print(f\"Final Feature Matrix Shape (validation): {X_val_combined.shape}\")  # (rows, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995\n",
      "F1 Score: 0.9995\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39503\n",
      "           1       1.00      1.00      1.00     36657\n",
      "\n",
      "    accuracy                           1.00     76160\n",
      "   macro avg       1.00      1.00      1.00     76160\n",
      "weighted avg       1.00      1.00      1.00     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_combined)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 med title som features (ud over main-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fillna(\"\") replaces NaN values with an empty string, preventing TfidfVectorizer from crashing. The empty string ensures that missing titles do not introduce errors but contribute zero influence in the TF-IDF matrix :))\n",
    "\n",
    "TF-IDF is generally better than just BOW for short texts like titles because it helps differentiate important words from common ones - this is relevant since we haven't preporcessed the text in the titles :)) \\\n",
    "Using title as features can help catch clickbait fake news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Matrix Shape (training): (609283, 15000)\n",
      "Final Feature Matrix Shape (validation): (76160, 15000)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values in the 'title' column with an empty string\n",
    "train_data['title'] = train_data['title'].fillna(\"\")\n",
    "validation_data['title'] = validation_data['title'].fillna(\"\")\n",
    "\n",
    "# Encode 'title' as a numerical feature\n",
    "vectorizer_title = TfidfVectorizer(max_features=5000)  # Convert title to TF-IDF\n",
    "X_train_title = vectorizer_title.fit_transform(train_data['title'])\n",
    "X_val_title = vectorizer_title.transform(validation_data['title'])\n",
    "\n",
    "# Combine text features and title features\n",
    "X_train_co_title = hstack([X_train_bow, X_train_title])                # Horizontally stack sparse matrices\n",
    "X_val_co_title = hstack([X_val_bow, X_val_title])                      # Horizontally stack sparse matrices\n",
    "\n",
    "# Check the resulting feature shape\n",
    "print(f\"Final Feature Matrix Shape (training): {X_train_co_title.shape}\")  # (rows, num_features)\n",
    "print(f\"Final Feature Matrix Shape (validation): {X_val_co_title.shape}\")  # (rows, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8678\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     39503\n",
      "           1       0.87      0.86      0.87     36657\n",
      "\n",
      "    accuracy                           0.87     76160\n",
      "   macro avg       0.87      0.87      0.87     76160\n",
      "weighted avg       0.87      0.87      0.87     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "model.fit(X_train_co_title, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_co_title)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stadig task 2 med title, her optimeret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8733\n",
      "F1 Score: 0.8689\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88     39503\n",
      "           1       0.87      0.87      0.87     36657\n",
      "\n",
      "    accuracy                           0.87     76160\n",
      "   macro avg       0.87      0.87      0.87     76160\n",
      "weighted avg       0.87      0.87      0.87     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2500, class_weight='balanced')\n",
    "model.fit(X_train_co_title, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_co_title)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3, preprocessing er gjort i Part1Task2 sammen med processing a det store dataset :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.read_csv('processed_scraped_articles.csv')                                       # Reads processed scraped data created earlier to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(713, 2)\n",
      "(609283,)\n",
      "(609996,)\n",
      "(609283,)\n",
      "(609996,)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate stemmed tokens from processed data to X_train\n",
    "X_train_extended = pd.concat([X_train, processed_data['stemmed_tokens']], ignore_index=True)\n",
    "\n",
    "# Concatenate type column (0) to y_train\n",
    "y_train_extended = pd.concat([y_train, processed_data['type']], ignore_index=True)\n",
    "\n",
    "# Control that concatenation happened correctly\n",
    "print(processed_data.shape)                        # should be (713, 2)\n",
    "print(X_train.shape)                               # should be (xx, )\n",
    "print(X_train_extended.shape)                      # should be (xx+713, )\n",
    "print(y_train.shape)                               # should be (xx, )\n",
    "print(y_train_extended.shape)                      # should be (xx+713, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609996, 10000)\n",
      "(76160, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Convert text to a bag-of-words representation\n",
    "vectorizer2 = CountVectorizer(max_features=10000)  # Use 10.000 most used words as features\n",
    "X_train_ext_bow = vectorizer2.fit_transform(X_train_extended)\n",
    "X_val_ext_bow = vectorizer2.transform(X_val)\n",
    "\n",
    "# Check shape of resulting matrix (should be (sample_size, 10000))\n",
    "print(X_train_ext_bow.shape)\n",
    "print(X_val_ext_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8426\n",
      "F1 Score: 0.8395\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85     39503\n",
      "           1       0.82      0.86      0.84     36657\n",
      "\n",
      "    accuracy                           0.84     76160\n",
      "   macro avg       0.84      0.84      0.84     76160\n",
      "weighted avg       0.84      0.84      0.84     76160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1500, class_weight='balanced') \n",
    "model.fit(X_train_ext_bow, y_train_extended)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_ext_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det hjalp kun meeeeeget lidt, men det er lidt som forventet når man smider 713 artikler på et training-set der indeholder 600.000 artikler :))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
