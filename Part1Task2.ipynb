{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block initializes the stopwords and stemmer used for text preprocessing in natural language processing (NLP) tasks.\n",
    "\n",
    "1. `stop_words = set(stopwords.words(\"english\"))`: \n",
    "   - This line loads the English stopwords from the NLTK library and converts them into a set. \n",
    "   - Stopwords are common words (such as 'and', 'the', 'is', etc.) that are often removed from text during preprocessing because they carry less meaningful information. \n",
    "   - By using a set, the lookup time for checking if a word is a stopword is optimized, as sets provide average O(1) time complexity for membership tests.\n",
    "\n",
    "2. `stemmer = PorterStemmer()`: \n",
    "   - This line creates an instance of the `PorterStemmer` class from the NLTK library. \n",
    "   - The Porter stemmer is an algorithm that reduces words to their root form (or stem) by removing suffixes. \n",
    "   - This process helps in standardizing words for analysis, as different forms of a word (e.g., 'running', 'ran', 'runs') can be reduced to a common base form (e.g., 'run').\n",
    "\n",
    "Overall, this code block sets up the necessary components for text preprocessing, specifically for removing common stopwords and stemming words to their root forms, which are essential steps in preparing text data for further analysis or modeling.\n",
    "\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))                        # load English stopwords from NLTK\n",
    "stemmer = PorterStemmer()                                           # create a new Porter stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing functions\n",
    "def clean_text(text):\n",
    "    text = text.lower()                                             # convert to lowercase\n",
    "    spaces = re.compile(r'\\s+')\n",
    "    text = spaces.sub(' ', text)                                    # substitute all white space characters (single or multiple occurences) with a single space\n",
    "\n",
    "    emails = re.compile(r'\\S+@\\S+\\.\\S+')\n",
    "    text = emails.sub('_EMAIL_', text)                              # substitute all found email addresses with _EMAIL_\n",
    "    urls = re.compile(r'http[s]?:\\/\\/\\S+|www\\.\\S+|\\S+\\.[a-z]+\\/\\S+|\\w+\\.(?:com|net|org)')\n",
    "    text = urls.sub('_URL_', text)                                  # substitute all found URLs with _URL_\n",
    "    dates = re.compile(r'''\n",
    "                       \\d{1,4}[-\\/]\\d{1,2}[-\\/]\\d{1,4}|\n",
    "                       \\d{1,2}\\ (?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[e]?|jul[y]?|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\\ \\d{,4}|\n",
    "                       (?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[e]?|jul[y]?|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)[,.]?\\ ?\\d{1,4}(?:th|st|nd|rd)?(?:,\\ \\d{4})?\n",
    "                       ''', re.VERBOSE)\n",
    "    text = dates.sub('_DATE_', text)                                # substitute all found dates with _DATE_\n",
    "    numbers = re.compile(r'\\d+(?:th|st|nd|rd)?')\n",
    "    text = numbers.sub('_NUM_', text)                               # substitute all remaining numbers with _NUM_\n",
    "    return text\n",
    "\n",
    "def remove_stopwords_and_stem(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_file = \"995,000_rows.csv\"\n",
    "processed_file = \"processed_fake_news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved a chunk of 7903 rows.\n",
      "Processed and saved a chunk of 7950 rows.\n",
      "Processed and saved a chunk of 7888 rows.\n",
      "Processed and saved a chunk of 7722 rows.\n",
      "Processed and saved a chunk of 7840 rows.\n",
      "Processed and saved a chunk of 7977 rows.\n",
      "Processed and saved a chunk of 7891 rows.\n",
      "Processed and saved a chunk of 7894 rows.\n",
      "Processed and saved a chunk of 7948 rows.\n",
      "Processed and saved a chunk of 7901 rows.\n",
      "Processed and saved a chunk of 7685 rows.\n",
      "Processed and saved a chunk of 7697 rows.\n",
      "Processed and saved a chunk of 7776 rows.\n",
      "Processed and saved a chunk of 7905 rows.\n",
      "Processed and saved a chunk of 8017 rows.\n",
      "Processed and saved a chunk of 7810 rows.\n",
      "Processed and saved a chunk of 7801 rows.\n",
      "Processed and saved a chunk of 7888 rows.\n",
      "Processed and saved a chunk of 7867 rows.\n",
      "Processed and saved a chunk of 7454 rows.\n",
      "Processed and saved a chunk of 7467 rows.\n",
      "Processed and saved a chunk of 7579 rows.\n",
      "Processed and saved a chunk of 7622 rows.\n",
      "Processed and saved a chunk of 7396 rows.\n",
      "Processed and saved a chunk of 7690 rows.\n",
      "Processed and saved a chunk of 7670 rows.\n",
      "Processed and saved a chunk of 7545 rows.\n",
      "Processed and saved a chunk of 7480 rows.\n",
      "Processed and saved a chunk of 7762 rows.\n",
      "Processed and saved a chunk of 7759 rows.\n",
      "Processed and saved a chunk of 7820 rows.\n",
      "Processed and saved a chunk of 7900 rows.\n",
      "Processed and saved a chunk of 7678 rows.\n",
      "Processed and saved a chunk of 7463 rows.\n",
      "Processed and saved a chunk of 7610 rows.\n",
      "Processed and saved a chunk of 7773 rows.\n",
      "Processed and saved a chunk of 7750 rows.\n",
      "Processed and saved a chunk of 7786 rows.\n",
      "Processed and saved a chunk of 7753 rows.\n",
      "Processed and saved a chunk of 7754 rows.\n",
      "Processed and saved a chunk of 7555 rows.\n",
      "Processed and saved a chunk of 7535 rows.\n",
      "Processed and saved a chunk of 7562 rows.\n",
      "Processed and saved a chunk of 7662 rows.\n",
      "Processed and saved a chunk of 7928 rows.\n",
      "Processed and saved a chunk of 7956 rows.\n",
      "Processed and saved a chunk of 7899 rows.\n",
      "Processed and saved a chunk of 7848 rows.\n",
      "Processed and saved a chunk of 7884 rows.\n",
      "Processed and saved a chunk of 7647 rows.\n",
      "Processed and saved a chunk of 7653 rows.\n",
      "Processed and saved a chunk of 7906 rows.\n",
      "Processed and saved a chunk of 7959 rows.\n",
      "Processed and saved a chunk of 7911 rows.\n",
      "Processed and saved a chunk of 7871 rows.\n",
      "Processed and saved a chunk of 7735 rows.\n",
      "Processed and saved a chunk of 7547 rows.\n",
      "Processed and saved a chunk of 7765 rows.\n",
      "Processed and saved a chunk of 7779 rows.\n",
      "Processed and saved a chunk of 7634 rows.\n",
      "Processed and saved a chunk of 7782 rows.\n",
      "Processed and saved a chunk of 7836 rows.\n",
      "Processed and saved a chunk of 7841 rows.\n",
      "Processed and saved a chunk of 7805 rows.\n",
      "Processed and saved a chunk of 7932 rows.\n",
      "Processed and saved a chunk of 7866 rows.\n",
      "Processed and saved a chunk of 8071 rows.\n",
      "Processed and saved a chunk of 7991 rows.\n",
      "Processed and saved a chunk of 7943 rows.\n",
      "Processed and saved a chunk of 7936 rows.\n",
      "Processed and saved a chunk of 7886 rows.\n",
      "Processed and saved a chunk of 7891 rows.\n",
      "Processed and saved a chunk of 7836 rows.\n",
      "Processed and saved a chunk of 7638 rows.\n",
      "Processed and saved a chunk of 7783 rows.\n",
      "Processed and saved a chunk of 7467 rows.\n",
      "Processed and saved a chunk of 7799 rows.\n",
      "Processed and saved a chunk of 7912 rows.\n",
      "Processed and saved a chunk of 7551 rows.\n",
      "Processed and saved a chunk of 7363 rows.\n",
      "Processed and saved a chunk of 7204 rows.\n",
      "Processed and saved a chunk of 7175 rows.\n",
      "Processed and saved a chunk of 7060 rows.\n",
      "Processed and saved a chunk of 6995 rows.\n",
      "Processed and saved a chunk of 6978 rows.\n",
      "Processed and saved a chunk of 7444 rows.\n",
      "Processed and saved a chunk of 7356 rows.\n",
      "Processed and saved a chunk of 7380 rows.\n",
      "Processed and saved a chunk of 6927 rows.\n",
      "Processed and saved a chunk of 6868 rows.\n",
      "Processed and saved a chunk of 6901 rows.\n",
      "Processed and saved a chunk of 6981 rows.\n",
      "Processed and saved a chunk of 7148 rows.\n",
      "Processed and saved a chunk of 7139 rows.\n",
      "Processed and saved a chunk of 7423 rows.\n",
      "Processed and saved a chunk of 7372 rows.\n",
      "Processed and saved a chunk of 7431 rows.\n",
      "Processed and saved a chunk of 7273 rows.\n",
      "Processed and saved a chunk of 7418 rows.\n",
      "Processed and saved a chunk of 3696 rows.\n",
      "Processing complete! Data saved to: processed_fake_news.csv\n"
     ]
    }
   ],
   "source": [
    "'''This code block reads a CSV file in chunks, processes each chunk to clean and preprocess the text data, and then saves the processed data to a new CSV file. It utilizes the pandas library for data manipulation and assumes the presence of a text preprocessing pipeline.\n",
    "'''\n",
    "# Read and process in chunks\n",
    "reader = pd.read_csv(input_file, usecols=['domain', 'type', 'url', 'content', 'title'], chunksize=10000)\n",
    "first_chunk = True  # Track the first chunk for writing header\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk.dropna(subset=['content', 'type'], inplace=True)                                    # drop rows with no content or type (/label)\n",
    "    chunk.drop(chunk[chunk['type'] == 'unknown'].index, inplace=True)                         # drop rows where 'type' is 'unknown'\n",
    "    chunk.drop(chunk[chunk['type'] == 'unreliable'].index, inplace=True)                      # drop rows where 'type' is 'unreliable'\n",
    "    chunk.drop_duplicates(subset=['content'], inplace=True)                                   # drop rows with duplicates in the 'content' column\n",
    "\n",
    "    # Apply text preprocessing pipeline\n",
    "    chunk[\"content\"] = chunk[\"content\"].apply(clean_text)                               # cleaning the text in the content column\n",
    "    chunk[\"content\"] = chunk[\"content\"].apply(word_tokenize)                             # tokenizing the text in the content column\n",
    "    chunk[\"stemmed_tokens\"] = chunk[\"content\"].apply(remove_stopwords_and_stem)          # removing stopwords and stemming the tokens \n",
    "\n",
    "    # Write and save processed chunk to csv file\n",
    "    chunk.to_csv(processed_file, mode=\"w\" if first_chunk else \"a\", index=False, header=first_chunk)\n",
    "    first_chunk = False # Ensure only the first chunk writes header\n",
    "\n",
    "    print(f\"Processed and saved a chunk of {len(chunk)} rows.\")\n",
    "\n",
    "print(\"Processing complete! Data saved to:\", processed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "scraped_data = \"CBS_bbc_scraped_articles.csv\"\n",
    "processed_data = \"processed_scraped_articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "0    [almost, _num_, ,, _num_, motorist, republ, ir...\n",
      "1    [freight, train, carri, highli, toxic, chemic,...\n",
      "Name: stemmed_tokens, dtype: object\n",
      "<class 'str'>\n",
      "0    almost _num_ , _num_ motorist republ ireland s...\n",
      "1    freight train carri highli toxic chemic benzen...\n",
      "Name: stemmed_tokens, dtype: object\n",
      "Processing complete! Data saved to: processed_scraped_articles.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block reads, processes, and saves scraped text data from a CSV file. It performs data cleaning, text preprocessing, and prepares the data for further analysis or modeling.\n",
    "\n",
    "1. `scraped_df = pd.read_csv(scraped_data, usecols=['text'])`:\n",
    "   - This line reads the scraped data from a CSV file specified by `scraped_data`, loading only the 'text' column into a pandas DataFrame called `scraped_df`.\n",
    "\n",
    "2. `scraped_df.dropna(subset=['text'], inplace=True)`:\n",
    "   - This line removes any rows from the DataFrame that have missing values (NaN) in the 'text' column, ensuring that only complete entries are processed.\n",
    "\n",
    "3. `scraped_df.drop_duplicates(subset=['text'], inplace=True)`:\n",
    "   - This line removes any duplicate rows based on the 'text' column, ensuring that each piece of text is unique.\n",
    "\n",
    "4. `scraped_df['text'] = scraped_df['text'].apply(clean_text)`:\n",
    "   - This line applies the `clean_text` function to the 'text' column, cleaning the text data by performing operations such as lowercasing, removing unwanted characters, and other preprocessing steps.\n",
    "\n",
    "5. `scraped_df['text'] = scraped_df['text'].apply(word_tokenize)`:\n",
    "   - This line tokenizes the cleaned text in the 'text' column, splitting it into individual words (tokens).\n",
    "\n",
    "6. `scraped_df['stemmed_tokens'] = scraped_df['text'].apply(remove_stopwords_and_stem)`:\n",
    "   - This line applies a function (assumed to be defined elsewhere) that removes stopwords and stems the tokens in the 'text' column, storing the results in a new column called 'stemmed_tokens'.\n",
    "\n",
    "7. `scraped_df['type'] = 0`:\n",
    "   - This line adds a new column called 'type' to the DataFrame, assigning a value of 0 (indicating 'reliable') to all rows. This could be used for classification purposes later.\n",
    "\n",
    "8. `scraped_df['stemmed_tokens'] = scraped_df['stemmed_tokens'].apply(lambda x: ' '.join(x))`:\n",
    "   - This line converts the lists of stemmed tokens in the 'stemmed_tokens' column into single strings, joining the tokens with spaces. This format is often required for text analysis tools like CountVectorizer.\n",
    "Overall, this code block efficiently processes scraped text data by cleaning, tokenizing, and stemming the text, while also preparing it for further analysis or modeling by saving it in a structured format.\n",
    "\"\"\"\n",
    "\n",
    "scraped_df = pd.read_csv(scraped_data, usecols=['text'])\n",
    "\n",
    "scraped_df.dropna(subset=['text'], inplace=True)                                    # drop rows with no text\n",
    "scraped_df.drop_duplicates(subset=['text'], inplace=True)                           # drop rows with duplicates in the 'text' column\n",
    "\n",
    "# Apply text preprocessing pipeline\n",
    "scraped_df['text'] = scraped_df['text'].apply(clean_text)                                   # cleaning the text in the text column\n",
    "scraped_df['text'] = scraped_df['text'].apply(word_tokenize)                                # tokenizing the text in the text column\n",
    "scraped_df['stemmed_tokens'] = scraped_df['text'].apply(remove_stopwords_and_stem)          # removing stopwords and stemming the tokens \n",
    "\n",
    "# Add 'type' column with value 0 (reliable) for all rows\n",
    "scraped_df['type'] = 0\n",
    "\n",
    "# Convert data in 'stemmed_tokens' column from list of strings\n",
    "# to a single string pr. article (as this is what e.g. CountVectorizer expects as input)\n",
    "print(type(scraped_df['stemmed_tokens'][0]))\n",
    "print(scraped_df['stemmed_tokens'].head(2))\n",
    "scraped_df['stemmed_tokens'] = scraped_df['stemmed_tokens'].apply(lambda x: ' '.join(x))\n",
    "print(type(scraped_df['stemmed_tokens'][0]))\n",
    "print(scraped_df['stemmed_tokens'].head(2))\n",
    "\n",
    "# Write and save processed data to csv file\n",
    "scraped_df.to_csv(processed_data, columns=['stemmed_tokens', 'type'], mode=\"w\", index=False, header=True)\n",
    "\n",
    "print(\"Processing complete! Data saved to:\", processed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating baseline model on the LIAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "liar_test = \"test.tsv\"\n",
    "liar_processed = \"liar_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "0    [build, wall, u.s.-mexico, border, take, liter...\n",
      "1    [wisconsin, pace, doubl, number, layoff, year, .]\n",
      "Name: stemmed_tokens, dtype: object\n",
      "<class 'str'>\n",
      "0    build wall u.s.-mexico border take liter year .\n",
      "1          wisconsin pace doubl number layoff year .\n",
      "Name: stemmed_tokens, dtype: object\n",
      "label\n",
      "half-true      265\n",
      "false          249\n",
      "mostly-true    241\n",
      "barely-true    212\n",
      "true           208\n",
      "pants-fire      92\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    449\n",
      "1    341\n",
      "Name: count, dtype: int64\n",
      "Processing complete! Data saved to: liar_test.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block reads, processes, and saves test data from the LIAR dataset, which contains statements labeled as true or false. It performs text preprocessing, filters relevant labels, and prepares the data for further analysis or modeling.\n",
    "\n",
    "1. `liar_df = pd.read_csv(liar_test, sep = '\\t', usecols=[1,2], names=['label', 'statement'])`:\n",
    "   - This line reads the LIAR dataset from a tab-separated values (TSV) file specified by `liar_test`. \n",
    "   - It loads only the second and third columns (index 1 and 2) and assigns them the names 'label' and 'statement', respectively.\n",
    "2. `liar_df['statement'] = liar_df['statement'].apply(clean_text)`:\n",
    "   - This line applies the `clean_text` function to the 'statement' column, cleaning the text data by performing operations such as lowercasing, removing unwanted characters, and other preprocessing steps.\n",
    "3. `liar_df['statement'] = liar_df['statement'].apply(word_tokenize)`:\n",
    "   - This line tokenizes the cleaned statements in the 'statement' column, splitting each statement into individual words (tokens).\n",
    "4. `liar_df['stemmed_tokens'] = liar_df['statement'].apply(remove_stopwords_and_stem)`:\n",
    "   - This line applies a function (assumed to be defined elsewhere) that removes stopwords and stems the tokens in the 'statement' column, storing the results in a new column called 'stemmed_tokens'.\n",
    "5. `liar_df['stemmed_tokens'] = liar_df['stemmed_tokens'].apply(lambda x: ' '.join(x))`:\n",
    "   - This line converts the lists of stemmed tokens in the 'stemmed_tokens' column into single strings, joining the tokens with spaces. This format is often required for text analysis tools like CountVectorizer.\n",
    "6. The following print statements are used for debugging and verification:\n",
    "   - `print(type(liar_df['stemmed_tokens'][0]))`: Displays the type of the first entry in the 'stemmed_tokens' column before conversion.\n",
    "   - `print(liar_df['stemmed_tokens'].head(2))`: Displays the first two entries in the 'stemmed_tokens' column before conversion.\n",
    "   - `print(type(liar_df['stemmed_tokens'][0]))`: Displays the type of the first entry in the 'stemmed_tokens' column after conversion.\n",
    "   - `print(liar_df['stemmed_tokens'].head(2))`: Displays the first two entries in the 'stemmed_tokens' column after conversion.\n",
    "7. `print(liar_df['label'].value_counts())`:\n",
    "   - This line prints an overview of the counts of each unique label in the 'label' column, providing insight into the distribution of true and false statements.\n",
    "8. `fake_lables = ['pants-fire', 'false']` and `reliable_lables = ['true', 'mostly-true']`:\n",
    "   - These lines define lists of labels that categorize statements as 'fake' or 'reliable'.\n",
    "9. `liar_df = liar_df[liar_df['label'].isin(fake_lables + reliable_lables)]`:\n",
    "   - This line filters the DataFrame to keep only the rows with labels that are either in the `fake_lables` or `reliable_lables` lists, ensuring that only relevant statements are retained.\n",
    "10. `liar_df['label'] = liar_df['label'].apply(lambda x: 1 if x in fake_lables else 0)`:\n",
    "    - This line converts the 'label' column to numerical values, assigning a value of 1 for 'fake' labels and 0 for 'reliable' labels.\n",
    "11. `print(liar_df['label'].value_counts())`:\n",
    "    - This line prints the count of statements grouped as 'fake' or 'reliable', providing a summary of the processed data.\n",
    "Overall, this code block efficiently processes test data from the LIAR dataset by cleaning, tokenizing, and stemming the statements, filtering relevant labels, and saving the processed data in a structured format for further analysis or modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "liar_df = pd.read_csv(liar_test, sep = '\\t', usecols=[1,2], names=['label', 'statement'])\n",
    "\n",
    "# Apply text preprocessing pipeline\n",
    "liar_df['statement'] = liar_df['statement'].apply(clean_text)                                   # cleaning the statements in the statement column\n",
    "liar_df['statement'] = liar_df['statement'].apply(word_tokenize)                                # tokenizing the statements in the statement column\n",
    "liar_df['stemmed_tokens'] = liar_df['statement'].apply(remove_stopwords_and_stem)               # removing stopwords and stemming the tokens \n",
    "\n",
    "# Convert data in 'stemmed_tokens' column from list of strings\n",
    "# to a single string pr. article (as this is what e.g. CountVectorizer expects as input)\n",
    "print(type(liar_df['stemmed_tokens'][0]))\n",
    "print(liar_df['stemmed_tokens'].head(2))\n",
    "liar_df['stemmed_tokens'] = liar_df['stemmed_tokens'].apply(lambda x: ' '.join(x))\n",
    "print(type(liar_df['stemmed_tokens'][0]))\n",
    "print(liar_df['stemmed_tokens'].head(2))\n",
    "\n",
    "\n",
    "# Print overview of occuring labels \n",
    "print(liar_df['label'].value_counts())\n",
    "\n",
    "fake_lables = ['pants-fire', 'false']\n",
    "reliable_lables = ['true', 'mostly-true']\n",
    "\n",
    "liar_df = liar_df[liar_df['label'].isin(fake_lables + reliable_lables)]                     # Keep only relevant labels\n",
    "liar_df['label'] = liar_df['label'].apply(lambda x: 1 if x in fake_lables else 0)             # Convert the 'label' column to numerical values\n",
    "\n",
    "print(liar_df['label'].value_counts())                                                      # Print the count of statements grouped as 'fake' or 'reliable'\n",
    "\n",
    "\n",
    "# Write and save processed data to csv file\n",
    "liar_df.to_csv(liar_processed, columns=['stemmed_tokens', 'label'], mode=\"w\", index=False, header=True)\n",
    "\n",
    "print(\"Processing complete! Data saved to:\", liar_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
