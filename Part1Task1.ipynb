{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block imports several libraries and modules that are commonly used for data manipulation, text processing, and visualization in Python.\n",
    "\n",
    "1. `pandas`: A powerful data manipulation and analysis library that provides data structures like DataFrames for handling structured data.\n",
    "2. `re`: A module for working with regular expressions, which are used for string searching and manipulation.\n",
    "3. `nltk`: The Natural Language Toolkit, a library for working with human language data (text). It provides tools for text processing, including tokenization, stopword removal, and stemming.\n",
    "   - `word_tokenize`: A function from NLTK that splits text into individual words (tokens).\n",
    "   - `stopwords`: A corpus from NLTK that contains common words (like 'and', 'the', etc.) that are often filtered out in text processing.\n",
    "   - `PorterStemmer`: A stemming algorithm that reduces words to their base or root form.\n",
    "4. `matplotlib.pyplot`: A plotting library used for creating static, animated, and interactive visualizations in Python.\n",
    "\n",
    "The commented-out lines at the end indicate that the user may need to download specific NLTK resources (stopwords and punkt tokenizer) for the code to function properly. These lines are currently inactive but can be uncommented to download the necessary datasets.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block defines functions and initializes variables for cleaning and processing text data, particularly for natural language processing (NLP) tasks.\n",
    "\n",
    "1. `stop_words`: A set containing English stopwords loaded from the NLTK library. Stopwords are common words that are often removed from text during processing to focus on more meaningful words.\n",
    "\n",
    "2. `stemmer`: An instance of the PorterStemmer class from NLTK, which is used to reduce words to their root form (stemming).\n",
    "\n",
    "3. `clean_text(text)`: A function that takes a string of text as input and performs several cleaning operations:\n",
    "   - Converts the text to lowercase to ensure uniformity.\n",
    "   - Uses a regular expression to replace multiple whitespace characters with a single space.\n",
    "   - Replaces email addresses in the text with the placeholder '_EMAIL_'.\n",
    "   - Replaces URLs in the text with the placeholder '_URL_'.\n",
    "   - Replaces dates in various formats with the placeholder '_DATE_' using a verbose regular expression.\n",
    "   - Replaces remaining numbers with the placeholder '_NUM_'.\n",
    "   - Returns the cleaned text.\n",
    "\n",
    "4. `remove_stopwords(tokens)`: A function that takes a list of tokens (words) as input and returns a new list containing only those words that are not in the `stop_words` set. This is useful for filtering out less meaningful words from the text.\n",
    "\n",
    "5. `stem_words(tokens)`: A function that takes a list of tokens as input and returns a new list where each word has been stemmed using the Porter stemmer. This reduces words to their base form, which can help in standardizing the text for analysis.\n",
    "\n",
    "Overall, this code is designed to preprocess text data by cleaning it, removing common stopwords, and stemming the remaining words, making it suitable for further analysis or modeling in NLP tasks.\n",
    "\"\"\"\n",
    "stop_words = set(stopwords.words(\"english\"))                        # load English stopwords from NLTK\n",
    "stemmer = PorterStemmer()                                           # create a new Porter stemmer\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()                                             # convert to lowercase\n",
    "    spaces = re.compile(r'\\s+')\n",
    "    text = spaces.sub(' ', text)                                    # substitute all white space characters (single or multiple occurences) with a single space\n",
    "\n",
    "    emails = re.compile(r'\\S+@\\S+\\.\\S+')\n",
    "    text = emails.sub('_EMAIL_', text)                              # substitute all found email addresses with _EMAIL_\n",
    "    urls = re.compile(r'http[s]?:\\/\\/\\S+|www\\.\\S+|\\S+\\.[a-z]+\\/\\S+|\\w+\\.(?:com|net|org)')\n",
    "    text = urls.sub('_URL_', text)                                  # substitute all found URLs with _URL_\n",
    "    dates = re.compile(r'''\n",
    "                       \\d{1,4}[-\\/]\\d{1,2}[-\\/]\\d{1,4}|\n",
    "                       \\d{1,2}\\ (?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[e]?|jul[y]?|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\\ \\d{,4}|\n",
    "                       (?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[e]?|jul[y]?|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)[,.]?\\ ?\\d{1,4}(?:th|st|nd|rd)?(?:,\\ \\d{4})?\n",
    "                       ''', re.VERBOSE)\n",
    "    text = dates.sub('_DATE_', text)                                # substitute all found dates with _DATE_\n",
    "    numbers = re.compile(r'\\d+(?:th|st|nd|rd)?')\n",
    "    text = numbers.sub('_NUM_', text)                               # substitute all remaining numbers with _NUM_\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stem_words(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   domain   250 non-null    object\n",
      " 1   type     238 non-null    object\n",
      " 2   url      250 non-null    object\n",
      " 3   content  250 non-null    object\n",
      " 4   title    250 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 9.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 215 entries, 1 to 246\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   domain   215 non-null    object\n",
      " 1   type     215 non-null    object\n",
      " 2   url      215 non-null    object\n",
      " 3   content  215 non-null    object\n",
      " 4   title    215 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 10.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block performs data loading and preprocessing on a CSV file containing news articles. It utilizes the pandas library for data manipulation.\n",
    "\n",
    "1. `sample = pd.read_csv(\"news_sample.csv\", usecols=['domain', 'type', 'url', 'content', 'title'])`: \n",
    "   - This line reads a CSV file named \"news_sample.csv\" into a pandas DataFrame called `sample`. \n",
    "   - The `usecols` parameter specifies that only the columns 'domain', 'type', 'url', 'content', and 'title' should be loaded from the CSV file.\n",
    "\n",
    "2. `print(sample.info())`: \n",
    "   - This line prints a summary of the DataFrame, including the number of entries, column names, non-null counts, and data types. This helps in understanding the structure and completeness of the data.\n",
    "\n",
    "3. `sample = sample.dropna(subset=['content', 'type'])`: \n",
    "   - This line removes any rows from the DataFrame that have missing values (NaN) in the 'content' or 'type' columns. This is important for ensuring that the dataset is complete for analysis.\n",
    "\n",
    "4. `sample = sample.drop(sample[sample['type'] == 'unknown'].index)`: \n",
    "   - This line drops any rows where the 'type' column has the value 'unknown'. This helps in cleaning the dataset by removing entries that do not have a clear classification.\n",
    "\n",
    "5. `sample = sample.drop(sample[sample['type'] == 'unreliable'].index)`: \n",
    "   - Similar to the previous line, this line removes rows where the 'type' is 'unreliable', further refining the dataset to include only reliable entries.\n",
    "\n",
    "6. `sample = sample.drop_duplicates(subset=['content'])`: \n",
    "   - This line removes any duplicate rows based on the 'content' column. This ensures that each piece of content in the dataset is unique, which is important for analysis and modeling.\n",
    "\n",
    "7. `print(sample.info())`: \n",
    "   - Finally, this line prints the updated summary of the DataFrame after the preprocessing steps, allowing the user to verify the changes made to the dataset.\n",
    "\n",
    "Overall, this code block is focused on loading a sample of news data, cleaning it by removing rows with missing or unreliable information, and ensuring that the content is unique for further analysis.\n",
    "\"\"\"\n",
    "\n",
    "sample = pd.read_csv(\"news_sample.csv\", usecols=['domain', 'type', 'url', 'content', 'title']) \n",
    "print(sample.info()) \n",
    "\n",
    "sample = sample.dropna(subset=['content', 'type'])                                    # drop rows with no content or type (/label)\n",
    "sample = sample.drop(sample[sample['type'] == 'unknown'].index)                       # drop rows where 'type' is 'unknown'\n",
    "sample = sample.drop(sample[sample['type'] == 'unreliable'].index)                    # drop rows where 'type' is 'unreliable'\n",
    "sample = sample.drop_duplicates(subset=['content'])                                   # drop rows with duplicates in the 'content' column\n",
    "\n",
    "print(sample.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
      "4  Donald Trump has the unnerving ability to abil...   \n",
      "6  Could you imagine waking up in the morgue? I f...   \n",
      "7  Citizen Journalist\\n\\nby N.Morgan Q has releas...   \n",
      "8  Usa Dollar Tanks On Mnuchin Statement That He ...   \n",
      "\n",
      "                                     cleaned_content  \\\n",
      "1  awakening of _NUM_ strands of dna – “reconnect...   \n",
      "4  donald trump has the unnerving ability to abil...   \n",
      "6  could you imagine waking up in the morgue? i f...   \n",
      "7  citizen journalist by n.morgan q has released ...   \n",
      "8  usa dollar tanks on mnuchin statement that he ...   \n",
      "\n",
      "                                              tokens  \\\n",
      "1  [awakening, of, _NUM_, strands, of, dna, –, “,...   \n",
      "4  [donald, trump, has, the, unnerving, ability, ...   \n",
      "6  [could, you, imagine, waking, up, in, the, mor...   \n",
      "7  [citizen, journalist, by, n.morgan, q, has, re...   \n",
      "8  [usa, dollar, tanks, on, mnuchin, statement, t...   \n",
      "\n",
      "                                 tokens_no_stopwords  \\\n",
      "1  [awakening, _NUM_, strands, dna, –, “, reconne...   \n",
      "4  [donald, trump, unnerving, ability, ability, c...   \n",
      "6  [could, imagine, waking, morgue, ?, one, would...   \n",
      "7  [citizen, journalist, n.morgan, q, released, s...   \n",
      "8  [usa, dollar, tanks, mnuchin, statement, wants...   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "1  [awaken, _num_, strand, dna, –, “, reconnect, ...  \n",
      "4  [donald, trump, unnerv, abil, abil, creat, rea...  \n",
      "6  [could, imagin, wake, morgu, ?, one, would, tr...  \n",
      "7  [citizen, journalist, n.morgan, q, releas, sev...  \n",
      "8  [usa, dollar, tank, mnuchin, statement, want, ...  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block performs a series of text preprocessing steps on the 'content' column of the DataFrame `sample`. It utilizes previously defined functions to clean, tokenize, remove stopwords, and stem the text data.\n",
    "\n",
    "1. `sample[\"cleaned_content\"] = sample[\"content\"].apply(clean_text)`:\n",
    "   - This line applies the `clean_text` function to each entry in the 'content' column of the DataFrame. \n",
    "   - The cleaned text is stored in a new column called 'cleaned_content', which contains the text after performing operations such as converting to lowercase, removing emails, URLs, dates, and numbers.\n",
    "\n",
    "2. `sample[\"tokens\"] = sample[\"cleaned_content\"].apply(word_tokenize)`:\n",
    "   - This line tokenizes the cleaned text in the 'cleaned_content' column using the `word_tokenize` function from NLTK. \n",
    "   - The resulting list of tokens (individual words) is stored in a new column called 'tokens'.\n",
    "\n",
    "3. `sample[\"tokens_no_stopwords\"] = sample[\"tokens\"].apply(remove_stopwords)`:\n",
    "   - This line applies the `remove_stopwords` function to the 'tokens' column, filtering out common stopwords from the list of tokens. \n",
    "   - The resulting list of tokens without stopwords is stored in a new column called 'tokens_no_stopwords'.\n",
    "\n",
    "4. `sample[\"stemmed_tokens\"] = sample[\"tokens_no_stopwords\"].apply(stem_words)`:\n",
    "   - This line applies the `stem_words` function to the 'tokens_no_stopwords' column, stemming each token to its root form. \n",
    "   - The stemmed tokens are stored in a new column called 'stemmed_tokens'.\n",
    "\n",
    "5. `print(sample[[\"content\", \"cleaned_content\", \"tokens\", \"tokens_no_stopwords\", \"stemmed_tokens\"]].head())`:\n",
    "   - This line prints a preview of the first few rows of the DataFrame, displaying the original 'content' along with the intermediate steps of the preprocessing pipeline: 'cleaned_content', 'tokens', 'tokens_no_stopwords', and 'stemmed_tokens'.\n",
    "   - This allows the user to visually inspect the transformations applied to the text data at each stage of the preprocessing process.\n",
    "\n",
    "Overall, this code block systematically processes the text data in the 'content' column, transforming it through cleaning, tokenization, stopword removal, and stemming, while providing a preview of the results at each step.\n",
    "\"\"\"\n",
    "sample[\"cleaned_content\"] = sample[\"content\"].apply(clean_text)                     # cleaning the text in the content column\n",
    "sample[\"tokens\"] = sample[\"cleaned_content\"].apply(word_tokenize)                   # tokenizing the text in the content column\n",
    "sample[\"tokens_no_stopwords\"] = sample[\"tokens\"].apply(remove_stopwords)            # removing stopwords from the tokens \n",
    "sample[\"stemmed_tokens\"] = sample[\"tokens_no_stopwords\"].apply(stem_words)          # stemming the tokens \n",
    " \n",
    "print(sample[[\"content\", \"cleaned_content\", \"tokens\", \"tokens_no_stopwords\", \"stemmed_tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary before removing stopwords: 15585\n",
      "Vocabulary after removing stopwords: 15440\n",
      "Reduction rate after removing stopwords: 0.93%\n",
      "Vocabulary after stemming: 10676\n",
      "Reduction rate after stemming: 30.85%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block calculates and reports various statistics related to the vocabulary of the text data in the DataFrame `sample`. It focuses on the effects of stopword removal and stemming on the vocabulary size.\n",
    "\n",
    "1. `size_tokenized = len(set(word for doc in sample[\"tokens\"] for word in doc))`:\n",
    "   - This line computes the size of the vocabulary (unique words) from the 'tokens' column, which contains the tokenized text. \n",
    "   - It uses a set comprehension to collect unique words across all documents and calculates the total number of unique tokens, storing the result in `size_tokenized`.\n",
    "\n",
    "2. `size_wo_stopwords = len(set(word for doc in sample[\"tokens_no_stopwords\"] for word in doc))`:\n",
    "   - This line calculates the size of the vocabulary after removing stopwords by applying the same logic to the 'tokens_no_stopwords' column. \n",
    "   - The result, representing the number of unique words without stopwords, is stored in `size_wo_stopwords`.\n",
    "\n",
    "3. `size_stemmed = len(set(word for doc in sample[\"stemmed_tokens\"] for word in doc))`:\n",
    "   - This line computes the size of the vocabulary after stemming by applying the same logic to the 'stemmed_tokens' column. \n",
    "   - The number of unique stemmed words is stored in `size_stemmed`.\n",
    "\n",
    "4. `stopword_reduction_rate = (size_tokenized - size_wo_stopwords) / size_tokenized * 100`:\n",
    "   - This line calculates the reduction rate in vocabulary size due to stopword removal. \n",
    "   - It computes the percentage decrease in unique words by comparing the vocabulary size before and after stopword removal, storing the result in `stopword_reduction_rate`.\n",
    "\n",
    "5. `stemmed_reduction_rate = (size_wo_stopwords - size_stemmed) / size_wo_stopwords * 100`:\n",
    "   - This line calculates the reduction rate in vocabulary size due to stemming. \n",
    "   - It computes the percentage decrease in unique words by comparing the vocabulary size before and after stemming, storing the result in `stemmed_reduction_rate`.\n",
    "\n",
    "6. The following print statements output the calculated statistics:\n",
    "   - `print(f\"Vocabulary before removing stopwords: {size_tokenized}\")`: Displays the number of unique tokens before stopword removal.\n",
    "   - `print(f\"Vocabulary after removing stopwords: {size_wo_stopwords}\")`: Displays the number of unique tokens after stopword removal.\n",
    "   - `print(f\"Reduction rate after removing stopwords: {stopword_reduction_rate:.2f}%\")`: Displays the percentage reduction in vocabulary size due to stopword removal.\n",
    "   - `print(f\"Vocabulary after stemming: {size_stemmed}\")`: Displays the number of unique stemmed tokens.\n",
    "   - `print(f\"Reduction rate after stemming: {stemmed_reduction_rate:.2f}%\")`: Displays the percentage reduction in vocabulary size due to stemming.\n",
    "\n",
    "Overall, this code block provides insights into the impact of text preprocessing techniques (stopword removal and stemming) on the vocabulary size of the dataset, helping to understand how these techniques affect the richness of the text data.\n",
    "\"\"\"\n",
    "size_tokenized = len(set(word for doc in sample[\"tokens\"] for word in doc))\n",
    "size_wo_stopwords = len(set(word for doc in sample[\"tokens_no_stopwords\"] for word in doc))\n",
    "size_stemmed = len(set(word for doc in sample[\"stemmed_tokens\"] for word in doc))\n",
    "\n",
    "stopword_reduction_rate = (size_tokenized - size_wo_stopwords) / size_tokenized * 100\n",
    "stemmed_reduction_rate = (size_wo_stopwords - size_stemmed) / size_wo_stopwords * 100\n",
    "\n",
    "print(f\"Vocabulary before removing stopwords: {size_tokenized}\")\n",
    "print(f\"Vocabulary after removing stopwords: {size_wo_stopwords}\")\n",
    "print(f\"Reduction rate after removing stopwords: {stopword_reduction_rate:.2f}%\")\n",
    "\n",
    "print(f\"Vocabulary after stemming: {size_stemmed}\")\n",
    "print(f\"Reduction rate after stemming: {stemmed_reduction_rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
