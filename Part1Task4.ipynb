{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_file = \"processed_fake_news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv(processed_file, usecols=['domain', 'type', 'url', 'title', 'stemmed_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761605, 5)\n",
      "type\n",
      "reliable                      213670\n",
      "political                     153516\n",
      "bias                          105406\n",
      "fake                           96400\n",
      "conspiracy                     78886\n",
      "rumor                          53893\n",
      "clickbait                      27114\n",
      "junksci                        12262\n",
      "satire                         11740\n",
      "hate                            8717\n",
      "2018-02-10 13:43:39.521661         1\n",
      "Name: count, dtype: int64\n",
      "(761604, 5)\n",
      "type\n",
      "0    394300\n",
      "1    367304\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(processed_df.shape)\n",
    "print(processed_df['type'].value_counts())\n",
    "\n",
    "fake_lables = ['bias', 'fake', 'conspiracy', 'rumor', 'junksci', 'hate', 'satire']\n",
    "reliable_lables = ['political', 'reliable', 'clickbait']\n",
    "\n",
    "processed_df = processed_df[processed_df['type'].isin(fake_lables + reliable_lables)]               # Keep only relevant types\n",
    "processed_df['type'] = processed_df['type'].apply(lambda x: 1 if x in fake_lables else 0)           # Convert the 'type' column to numerical values\n",
    "\n",
    "print(processed_df.shape)\n",
    "print(processed_df['type'].value_counts())                                                # Print the count of articles grouped as 'fake' or 'reliable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Convert data in 'stemmed_tokens' column from string to list of strings\n",
    "# and then to a single string pr. article (as this is what e.g. CountVectorizer expects as input)\n",
    "print(type(processed_df['stemmed_tokens'][0]))\n",
    "processed_df['stemmed_tokens']= processed_df['stemmed_tokens'].apply(ast.literal_eval)\n",
    "print(type(processed_df['stemmed_tokens'][0]))\n",
    "processed_df['stemmed_tokens'] = processed_df['stemmed_tokens'].apply(lambda x: ' '.join(x))\n",
    "print(type(processed_df['stemmed_tokens'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation-Test split complete! Data saved.\n"
     ]
    }
   ],
   "source": [
    "# Define X (features) and y (target)\n",
    "X = processed_df[['domain', 'url', 'title', 'stemmed_tokens']]  # Features\n",
    "y = processed_df['type']     # Target labels\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test data in half to get Validation data\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save training, validation and test sets\n",
    "train_processed_data = X_train.assign(type=y_train)\n",
    "test_processed_data = X_test.assign(type=y_test)\n",
    "val_processed_data = X_val.assign(type=y_val)\n",
    "\n",
    "train_processed_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_processed_data.to_csv(\"test_data.csv\", index=False)\n",
    "val_processed_data.to_csv(\"validation_data.csv\", index=False)\n",
    "print(\"Train-Validation-Test split complete! Data saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
